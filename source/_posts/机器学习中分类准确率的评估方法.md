---
title: 机器学习中分类准确率的评估方法
id: 2
categories:
  - Machine Learning
date: 2016-04-09 15:01:48
tags:
  - Machine Learning
mathjax: true
---

对机器学习的分类结果进行分析是一个很重要的过程，之前一直忽略了这一个过程，一直到使用了Scikit-learn之后才发现有一堆不懂的名词需要学习。下面主要解释下混淆矩阵、准确率、召回率、f1-score等概念。这些概念其实也是模式识别和信息检索里面经常碰到的东西。

## 混淆矩阵（Confusion Matrix）

混淆矩阵其实很好理解，就是把预测值和实际值写在同一个矩阵里。假设总共需要分为两类，那么混淆矩阵就是2x2的大小。每一行就是每一类的实际值，每一列就代表的是每一类的预测值。具体含义见下面的表格：

||预测类1|预测类2|预测类3|
|-|-|-|-|
|实际类1|43|5|2|
|实际类2|2|45|3|
|实际类3|0|1|49

比方下面这个混淆矩阵：
```
[515  34]
[ 80 262]
```
表达的含义是对于一个０１的二分类问题，实际值是０且预测值也为０的有515个，实际值是０但预测值为１的有34个，实际值是１但预测值也为０的有80个，实际值是１但预测值为１的有262个。

## Accuracy、Recall、F1-score的含义

准确率和召回率是最常用的评估方法，听上去玄乎其实很简单。

**准确率**是指对于预测而言，我的预测正确的概率。比如上面的那个混淆矩阵表示的结果，预测值为０的准确率就是515/(515+80)=0.87。

**召回率**是指对于实际而言，我的实际结果能够被正确预测出来的概率。比如上面的混淆矩阵中，实际值为０的召回率就是515/(515+34)=0.94

分出这两个判断标准也是有着实际的重要意义的。

比如通常我们在判断正确率的时候，用Accuracy表示就可以了，但是如果我们面对的是类似地震的预测时，我们并不特别在意他实际的准确率，宁可多预警几次来避免大的损失。此时召回率就显得特别重要了。

最后**F1-score**其实是准确率和召回率的综合考量，$f1score=\frac{2\*Accuracy\*Recall}{Accuracy+Recall}$。

## 相关参考

[机器学习 F1-Score, recall, precision](http://www.w2bc.com/Article/88963)
[召回率 Recall、精确度Precision、准确率Accuracy、虚警、漏警等分类判定指标](http://blog.sina.com.cn/s/blog_900690c60101czyo.html)
[准确率(Accuracy), 精确率(Precision), 召回率(Recall)和F1-Measure](https://argcv.com/articles/1036.c)
