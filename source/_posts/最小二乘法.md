---
title: 最小二乘法
id: 1
categories:
  - Machine Learning
date: 2016-02-22 22:49:48
tags:
  - Machine Learning
mathjax: true
---

最小二乘法，说白了其实就是解决线性回归问题的一个算法。这个算法最早是由高斯和勒让德分别独立发现的，也是当今十分常见的线性拟合算法，并不复杂。

我们常用的最小二乘法有两种，一种是普通方程表示的简单线性拟合问题，另一种是矩阵表示的高维度的线性拟合问题。

## 普通最小二乘法

他解决的基本问题其实就是给定一些数对$(x\_1,y\_1),(x\_2,y\_2),(x\_3,y\_3),....$，让你求出参数$\beta\_0,\beta\_1$，使得直线$y=\beta\_0+\beta\_1 x$能够最好的拟合这个数据集，也就是使得他的平方损失函数取到最小值，即
$$Q=\underset{i=1}{\overset{n}{\sum}} (y\_i-\beta\_0-\beta\_1x\_i)^2$$
最小。

为了取得最小值，我们分别对$\beta\_0,\beta\_1$求偏导，并使之为０。

$$
\left\\{\begin{aligned}&\frac{\partial Q}{\partial \beta\_0}=-2\underset{i=1}{\overset{n}{\sum}}(y\_i-\beta\_0-\beta\_1 x\_i)=0\\\\& \frac{\partial Q}{\partial \beta\_1}=-2\underset{i=1}{\overset{n}{\sum}}x\_i(y\_i-\beta\_0-\beta\_1 x\_i)=0\end{aligned}\right.
$$

解得：

$$\left\\{\begin{aligned}&\beta\_0=\frac{n\sum x\_iy\_i-\sum x\_i\sum y\_i}{n\sum x\_i^2-(\sum x\_i)^2}\\\\& \beta\_1=\frac{\sum x\_i^2\sum y\_i-\sum x\_i\sum x\_iy\_i}{n\sum x\_i^2-(\sum x\_i)^2}\end{aligned}\right.$$

套用这个公式得到的参数$\beta\_0,\beta\_1$就是最好的拟合参数了。


## 矩阵最小二乘法

用矩阵表示的最小二乘法则更加方便，能够用非常简单的矩阵形式进行计算，而且能拟合多维度的线性方程。

对于线性回归，我们要做的事情其实可以近似等同于解线性方程AX=Y，其中A是m*n的矩阵，X,Y是1*m的矩阵。m是数据的对数，n是数据的维数加１(因为还有常数)，而且n应该小于m。

当然，这个方程是没有解得，不过我们可以通过数值运算计算出一个解，可以证明这个解就是平方损失函数最小的解。

$$X=(A*A^T)^{-1}A^TY$$

解出的X就是线性回归函数的系数矩阵。

